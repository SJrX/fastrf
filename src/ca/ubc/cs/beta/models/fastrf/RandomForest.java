package ca.ubc.cs.beta.models.fastrf;

import java.io.File;
import java.io.FileInputStream;
import java.io.FileOutputStream;
import java.io.IOException;
import java.io.ObjectInputStream;
import java.io.ObjectOutputStream;
import java.util.*;

import ca.ubc.cs.beta.models.fastrf.utils.CsvToDataConverter;
import ca.ubc.cs.beta.models.fastrf.utils.RfData;
import ca.ubc.cs.beta.models.fastrf.utils.Utils;

public strictfp class RandomForest implements java.io.Serializable {
    private static final long serialVersionUID = 5204746081205095705L;
    
    public int numTrees;
    public Regtree[] Trees;
    public int logModel;
    public static final double MIN_VARIANCE_RESULT = -1 * Math.pow(10,-6);
    public double minVariance;

	private RegtreeBuildParams buildParams;
    
    public RandomForest(int numtrees, RegtreeBuildParams buildParams) {
        if (numtrees <= 0) {
            throw new RuntimeException("Invalid number of regression trees in forest: " + numtrees);
        }
        this.logModel = buildParams.logModel;
        numTrees = numtrees;
        Trees = new Regtree[numtrees];
        this.minVariance = buildParams.minVariance;
        this.buildParams = buildParams;
    }
    
	/*
	 *  Builds an RF with standard parameters from an RfData object.
	 */
	public static RandomForest buildRf(RfData trainData){
		//=== Read inputs from .csv file and learn RF.
		RegtreeBuildParams regTreeBuildParams = new RegtreeBuildParams(true, 10, trainData.getCatDomainSizes());
		RandomForest rf = RandomForest.learnModel(10, trainData.getTheta(), trainData.getX(), trainData.getTheta_inst_idxs(), trainData.getY(), regTreeBuildParams);
		return rf;
	}    
	
	/*
	 *  Builds an RF with standard parameters from an RfData object.
	 */
	public static RandomForest buildRf(RfData trainData, int numTrees){
		//=== Read inputs from .csv file and learn RF.
		RegtreeBuildParams regTreeBuildParams = new RegtreeBuildParams(true, 10, trainData.getCatDomainSizes());
		RandomForest rf = RandomForest.learnModel(numTrees, trainData.getTheta(), trainData.getX(), trainData.getTheta_inst_idxs(), trainData.getY(), regTreeBuildParams);
		return rf;
	}


	/*
	 *  Builds a deterministic RF that perfectly reproduces the training response values.
	 */
	public static RandomForest buildDeterministicRf(RfData trainData){
		//=== Read inputs from .csv file and learn RF.
		RegtreeBuildParams regTreeBuildParams = new RegtreeBuildParams(false, 1, 1.0, trainData.getCatDomainSizes());
		RandomForest rf = RandomForest.learnModel(1, trainData.getTheta(), trainData.getX(), trainData.getTheta_inst_idxs(), trainData.getY(), regTreeBuildParams);
		return rf;
	}

	
	
	/*
	 *  Builds the RF from a CSV file.
	 */
	public static RandomForest buildRfFromCSV(String csvFileName, int[] thetaColIdxs, int[] xColIdxs, int yColIdx, int[] catColIdxs) throws IOException{
		CsvToDataConverter converter = new CsvToDataConverter(csvFileName, thetaColIdxs, xColIdxs, yColIdx, catColIdxs);
		RfData trainData = converter.readDataFromCsvFile(csvFileName);
		return buildRf(trainData);
	}
	

    public boolean equals(Object o)
    {
    	
    	if(o instanceof RandomForest)
    	{
    		RandomForest rf = (RandomForest) o;
    		
    		if(numTrees != rf.numTrees) return false;
    		if(logModel != rf.logModel) return false;
    		return Arrays.equals(Trees, rf.Trees);    		
    		
    	} 
    	return false;
    }
    
    
    public RegtreeBuildParams getBuildParams() {
		return buildParams;
	}

	public int hashCode()
    {
    	return logModel ^ 2*numTrees ^ Arrays.deepHashCode(Trees);
    }
    
    public int matlabHashCode()
    {
    	return Math.abs(hashCode()) % 32452867;
    }

    /**
     *
     * N - Number of configurations 
     * K - Number of parameters for a configuration 
     * M - Number of instances
     * L - Number of features for an instance
     * P - Number of Runs preformed
     *
     * @param numTrees - Number of trees in the Random Forest
     * @param allTheta - N x K matrix of parameter values [ Each row is a configuration, each entry in a row represents the value for that parameter (in that configuration)]
     * @param allX - M x L matrix of instance features [ Each row is all the features for a single instance, each entry in a row represents the value of that feature].
     * @param theta_inst_idxs - P x 2 - Pairs of indexes ( A,B) where A points to a row in allTheta, and B points to a row in allX [ So an configuration, instance pair]. 
     * @param y - P x 1 - Response values for the corresponding pairs in theta_inst_idxes.
     * @param params - 
     *
     */
    public static RandomForest learnModel(int numTrees, double[][] allTheta, double[][] allX, int[][] theta_inst_idxs, double[] y, RegtreeBuildParams params) {
        Random r = params.random;
        if (r == null) {
            r = new Random();
            if (params.seed != -1) {
                r.setSeed(params.seed);
            }
        }        
        
        int N = y.length;
        
        //=== Assign data to each tree, with bootstrapping if set so in the regtree build params.
        int[][] dataIdxs = new int[numTrees][N];
        if (params.doBootstrapping){
	        for (int i = 0; i < numTrees; i++) {
	            for (int j = 0; j < N; j++) {
	                dataIdxs[i][j] = r.nextInt(N); // standard bootstrapping: draw a random data point, with repetitions.
	            }
	        }
        } else {
	        for (int i = 0; i < numTrees; i++) {
	            for (int j = 0; j < N; j++) {
	                dataIdxs[i][j] = j; // no bootstrapping: every tree gets every data point.
	            }
	        }        	
        }

        return learnModel(numTrees, allTheta, allX, theta_inst_idxs, y, dataIdxs, params);
    }
    
    
    /**
    *
    * N - Number of configurations 
    * K - Number of parameters for a configuration 
    * M - Number of instances
    * L - Number of features for an instance
    * P - Number of Runs preformed
    *
    * @param numTrees - Number of trees in the Random Forest
    * @param allTheta - N x K matrix of parameter values [ Each row is a configuration, each entry in a row represents the value for that parameter (in that configuration)]
    * @param allX - M x L matrix of instance features [ Each row is all the features for a single instance, each entry in a row represents the value of that feature].
    * @param theta_inst_idxs - P x 2 - Pairs of indexes ( A,B) where A points to a row in allTheta, and B points to a row in allX [ So an configuration, instance pair]. 
    * @param y - P x 1 - Response values for the corresponding pairs in theta_inst_idxes.
    * @param params - 
    *
    */
   public static RandomForest learnModelImputedValues(int numTrees, double[][] allTheta, double[][] allX, int[][] theta_inst_idxs, double[][] y, RegtreeBuildParams params) {
       Random r = params.random;
       if (r == null) {
           r = new Random();
           if (params.seed != -1) {
               r.setSeed(params.seed);
           }
       }        
       
       
       
       int N = y.length;
       
       // Do bootstrap sampling for data for each tree.
       int[][] dataIdxs = new int[numTrees][N];
       for (int i = 0; i < numTrees; i++) {
           for (int j = 0; j < N; j++) {
               dataIdxs[i][j] = r.nextInt(N);
           }
       }
       
   
       return learnModelImputedValues(numTrees, allTheta, allX, theta_inst_idxs, y, dataIdxs, params);
   }

    /**
     * Learns a random forest.
     * @param numTrees: number of trees to use.
     * @param allTheta: matrix of all of the configurations. Dimensionality: #configurations x #parameters per config
     * @param allX: matrix of features for all of the instances. Dimensionality: #instances x #features per instance
     * @param theta_inst_idxs: indices into allTheta and allX. Dimensionality: Nx2. This specifies for each input data point
     *                   which theta to use and which X to use. I.e., the i'th data point for the regression tree uses
     *                   the parameters allTheta[dataIdxs[i][1]] and the features allX[dataIdxs[i][2]]. The corresponding 
     *                   response values is y[i]. This is done to reduce the memory over a representation of the design
     *                   matrix as N x (#parameters + #features). 
     * @param y: vector of response values. Size: N
     * @param dataIdxs: Dimensions: #trees x #data points for each tree (typically in random forests taken to be equal to N).
     * @param params:
     * @see RegtreeFit#fit
     */
    public static RandomForest learnModel(int numTrees, double[][] allTheta, double[][] allX, int[][] theta_inst_idxs, double[] y, int[][] dataIdxs, RegtreeBuildParams params) {

       /**
        * TODO Add validaiton for index errors
        */

	
        if (dataIdxs.length != numTrees) {
            throw new RuntimeException("length(dataIdxs) must be equal to numtrees.");
        }
    
        /* 
         * Collect the bootstrapped data for each tree as specified by the indices to be used for each tree in dataIdxs.
         */
        RandomForest rf = new RandomForest(numTrees, params);
        for (int i = 0; i < numTrees; i++) {
            int N = dataIdxs[i].length;
            int[][] this_theta_inst_idxs = new int[N][];
            double[] thisy = new double[N];
            for (int j=0; j<N; j++) {
                int idx = dataIdxs[i][j];
                this_theta_inst_idxs[j] = theta_inst_idxs[idx];
                thisy[j] = y[idx];
            }
            rf.Trees[i] = RegtreeFit.fit(allTheta, allX, this_theta_inst_idxs, thisy, params);
        }
        return rf;
    }
    

    /**
     * Learns a random forest. Almost identical to the standard learnModel, except that we specify directly the y values to use in each tree. 
     * This is used for imputing values separately for each tree to deal with censored data.  
     * @param numTrees: number of trees to use.
     * @param allTheta: matrix of all of the configurations. Dimensionality: #configurations x #parameters per config
     * @param allX: matrix of features for all of the instances. Dimensionality: #instances x #features per instance
     * @param theta_inst_idxs: indices into allTheta and allX. Dimensionality: Nx2. This specifies for each input data point
     *                   which theta to use and which X to use. I.e., the i'th data point for the regression tree uses
     *                   the parameters allTheta[dataIdxs[i][1]] and the features allX[dataIdxs[i][2]]. The corresponding 
     *                   response values is y[i]. This is done to reduce the memory over a representation of the design
     *                   matrix as N x (#parameters + #features). 
     * @param y: vector of imputed response values, separately for each tree. Size: #trees x #data points for each tree
     * @param dataIdxs: Dimensions: #trees x #data points for each tree (typically in random forests taken to be equal to N).
     * @param params:
     * @see RegtreeFit#fit
     */
    @SuppressWarnings("unused")
	public static RandomForest learnModelImputedValues(int numTrees, double[][] allTheta, double[][] allX, int[][] theta_inst_idxs, double[][] y, int[][] dataIdxs, RegtreeBuildParams params) {

       /**
        * TODO Add validaiton for index errors
        */
        if (dataIdxs.length != numTrees) {
            throw new RuntimeException("length(dataIdxs) must be equal to numtrees.");
        }
    

        RandomForest rf = new RandomForest(numTrees, params);
        for (int i = 0; i < numTrees; i++) {
            int N = dataIdxs[i].length;
            int[][] this_theta_inst_idxs = new int[N][];
            double[] thisy = new double[N];
            for (int j=0; j<N; j++) {
                int idx = dataIdxs[i][j];
                this_theta_inst_idxs[j] = theta_inst_idxs[idx];
                //thisy[j] = y[i][j];
            }
            try {
            rf.Trees[i] = RegtreeFit.fit(allTheta, allX, this_theta_inst_idxs, y[i], params);
            } catch(RuntimeException e)
            {
            	throw e;
            }
        }
        return rf;
    }
    
    
    /**
     * Propogates data points down the regtree and returns a numtrees*X.length vector of node #s
     * specifying which node each data point falls into.
     * @param X a numdatapoints*numvars matrix
     */
    public static int[][] fwd(RandomForest forest, double[][] X) {
        int[][] retn = new int[forest.numTrees][X.length];
        for (int i=0; i < forest.numTrees; i++) {
            int[] result = RegtreeFwd.fwd(forest.Trees[i], X);
            System.arraycopy(result, 0, retn[i], 0, result.length);
        }
        return retn;
    }
    
    /**
     * Gets a prediction for the given instantiations of the input dimensions (number of columns in allTheta + number of columns in allX)
     * @return a matrix of size X.length*2 where index (i,0) is the prediction for X[i]
     * and (i,1) is the variance of that prediction. See Matlab code for how var is calculated.
     */
    public static double[][] apply(RandomForest forest, double[][] X) {
		double[][] retn = new double[X.length][2]; // mean, var
        for (int i=0; i < forest.numTrees; i++) {
            int[] result = RegtreeFwd.fwd(forest.Trees[i], X);
            for (int j=0; j < X.length; j++) {
				double pred = forest.Trees[i].nodepred[result[j]];
				double var = forest.Trees[i].nodevar[result[j]];
				
                if (forest.logModel>0) {
                	
                	if(forest.buildParams.brokenVarianceCalculation)
                	{
                		pred = Math.log10(pred);
                	} else
                	{
                		double test_mu_n = pred;
	    	            double test_var_n = var;
	    	            
	    	            double var_ln = Math.log(test_var_n/(test_mu_n*test_mu_n) + 1);
	    	            double	mu_ln = Math.log(test_mu_n) - var_ln/2;
	    	            
	    	            double var_l10 = var_ln / Math.log(10) / Math.log(10);
	    	            double mu_l10 = mu_ln / Math.log(10); 
	    	            
	    	            pred = mu_l10;
	    	            var = var_l10;
                	}
                }
                retn[j][0] += pred;
                retn[j][1] += var+pred*pred;
            }
        }

        for (int i=0; i < X.length; i++) {
            retn[i][0] /= forest.numTrees;
            retn[i][1] /= forest.numTrees;
            retn[i][1] -= retn[i][0]*retn[i][0];
            retn[i][1] = retn[i][1] * ( ((double) forest.numTrees)/Math.max(1, forest.numTrees-1));

            
            if(retn[i][1] < MIN_VARIANCE_RESULT)
            {
            	System.err.println("[WARN]: Variance is less than " + MIN_VARIANCE_RESULT + " > " + retn[i][1]);
            	assert(retn[i][1] > MIN_VARIANCE_RESULT); //Assert negative variance only comes from numerical issues (and they shouldn't make it too small)
            }
            retn[i][1] = Math.max(forest.minVariance, retn[i][1]);

        }
        
        
        return retn;
    }
    
    public static double round(double val)
    {
    	float fval = (float) val;
    	
    	int bits = Float.floatToRawIntBits(fval);
    	bits = bits & 0xFFFFF800;
    	float nval = Float.intBitsToFloat(bits);
    	
    	//System.out.println(fval + "=>" + nval);
    	return nval;
    }
    
    public static double[][] applyRoundedMarginal(RandomForest forest, int[] tree_idxs_used, double[][] Theta) {
    	double[][] results = applyMarginal(forest, tree_idxs_used, Theta);
    	for(int i=0; i < results.length; i++)
    		for(int j=0; j < results[i].length; j++)
    		{
    			double result = results[i][j];
    			
    			results[i][j] = round(result);
    		}
        return results;
    }
    
    public static double[][] applyRoundedMarginal(RandomForest forest, int[] tree_idxs_used, double[][] Theta, double[][] X) {
    	double[][] results = applyMarginal(forest, tree_idxs_used, Theta, X);
    	for(int i=0; i < results.length; i++)
    		for(int j=0; j < results[i].length; j++)
    		{
    			double result = results[i][j];
    			
    			results[i][j] = round(result);
    		}
        return results;
    }
    
    	/**
	 * Classifies the given instantiations of features
	 * @return a matrix of size X.length where index i contains the
	 * most popular response for X[i]
	 */
	public static double[] classify(RandomForest forest, double[][] X) {
		// This currently uses numTrees*X.length memory in order to
		// take advantage of batch forwarding of Xs. 
		// There might be some way of reducing memory but I haven't thought about it yet.
		double[][] votes = new double[X.length][forest.numTrees];
		for (int i=0; i < forest.numTrees; i++) {
			double[] res = Regtree.classify(forest.Trees[i], X);
			for (int j=0; j < res.length; j++) {
				votes[j][i] = res[j];
			}
		}
    
		double[] retn = new double[X.length];
		for (int i=0; i < X.length; i++) {
			double[] best = Utils.mode(votes[i]);
			retn[i] = best[(int)(Math.random()*best.length)];
		}
		return retn;
	}
    
    public static double[][] marginalTreePredictions(RandomForest forest, int[] tree_idxs_used, double[][] Theta) {
    	return marginalTreePredictions(forest, tree_idxs_used, Theta, null);
    }
	
    /**
     * Gets a prediction for each of the given configurations Theta, marginal across all the entries in the X used for training.
     * @return a matrix of size Theta.length*2 where index (i,0) is the prediction for Theta[i]
     */
	public static double[][] applyMarginal(RandomForest forest, int[] tree_idxs_used, double[][] Theta) {
    	return applyMarginal(forest, tree_idxs_used, Theta, null);
    }
    
    /**
     * Gets a prediction for each of the given configurations Theta, marginal across the entries in X.
     * @return a matrix of size Theta.length*2 where index (i,0) is the prediction for Theta[i]
     * and (i,1) is the variance of that prediction. See Matlab code for how var is calculated.
     * @see RegtreeFwd#marginalFwd
     */
    public static double[][] applyMarginal(RandomForest forest, int[] tree_idxs_used, double[][] Theta, double[][] X) {
        int nTheta = Theta.length, nTrees = tree_idxs_used.length;
		double[][] retn = new double[nTheta][2]; // mean, var
        
        for (int i=0; i < nTrees; i++) {
            Object[] result = RegtreeFwd.marginalFwd(forest.Trees[tree_idxs_used[i]], Theta, X);
            double[] preds = (double[])result[0];
            double[] vars = (double[])result[1];

            for (int j=0; j < nTheta; j++) {
                double pred = preds[j];
                if (forest.logModel>0) {
                    pred = Math.log10(pred);
                }
                retn[j][0] += pred;
                retn[j][1] += vars[j]+pred*pred;

            }
        }
        
        for (int i=0; i < nTheta; i++) {
            retn[i][0] /= nTrees;
            retn[i][1] /= nTrees;
			retn[i][1] -= retn[i][0]*retn[i][0];
            retn[i][1] = retn[i][1] * ((forest.numTrees+0.0)/Math.max(1, forest.numTrees-1));
            
            if(retn[i][1] < MIN_VARIANCE_RESULT) 
            {
            	System.err.println("[WARN]: Variance is less than " + MIN_VARIANCE_RESULT + " > " + retn[i][1]);
            	assert(retn[i][1] > MIN_VARIANCE_RESULT); //Assert negative variance only comes from numerical issues (and they shouldn't make it too small)
            }
           
            retn[i][1] = Math.max(forest.minVariance, retn[i][1]);
        }
        return retn;
    }
    
    /**
     * Gets the individual tree predictions for the given configurations and instances
     * @return a matrix of size Theta.length*B where index (i,b) is the prediction for Theta[i] of tree b.
     * @see RegtreeFwd#marginalFwd
     */
    public static double[][] marginalTreePredictions(RandomForest forest, int[] tree_idxs_used, double[][] Theta, double[][] X) {
        int nTheta = Theta.length, nTrees = tree_idxs_used.length;
		double[][] retn = new double[nTheta][nTrees];
        
        for (int i=0; i < nTrees; i++) {
            Object[] result = RegtreeFwd.marginalFwd(forest.Trees[tree_idxs_used[i]], Theta, X);
            double[] preds = (double[])result[0];
            //double[] vars = (double[])result[1];

            for (int j=0; j < nTheta; j++) {
                double pred = preds[j];
                if (forest.logModel>0) {
                    pred = Math.log10(pred);
                }
                retn[j][i] = pred;
                
//                if(Theta.length == 1)
//                {
//                	System.out.println(" Tree " + i + " predicted " + pred);
//                }
            }
        }
        return retn;
    }
    
    /** 
     * Prepares the random forest for marginal predictions.
     * @see RegtreeFwd#preprocess_inst_splits
     */
    public static RandomForest preprocessForest(RandomForest forest, double[][] X) {
        RandomForest prepared = new RandomForest(forest.numTrees,forest.buildParams);
        for (int i=0; i < forest.numTrees; i++) {
            prepared.Trees[i] = RegtreeFwd.preprocess_inst_splits(forest.Trees[i], X);
        }
        return prepared;
    }

	/** 
	 * Prepares the random forest for classification
	 * @see RegtreeFwd#preprocess_for_classification
	 */
	public static void preprocessForestForClassification(RandomForest forest) {
		for (int i=0; i < forest.numTrees; i++) {
			RegtreeFwd.preprocess_for_classification(forest.Trees[i]);
		}
	}
}
